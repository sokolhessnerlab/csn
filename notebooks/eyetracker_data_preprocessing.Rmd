---
title: "Eyetracker Data Preprocessing"
author: "Ari Dyckovsky"
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, 
                    encoding = encoding,
                    output_dir = "../output/notebooks") })
---

```{r, notebook-configuration, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  strip.white = TRUE,
  #tidy = TRUE,
  highlight = TRUE,
  warning = FALSE,
  message = FALSE
)

make_pretty_df <- function(df) {
  if (isTRUE(getOption('knitr.in.progress'))) {
    knitr::kable(df, "simple", format.args = list(big.mark = ",", scientific = FALSE))
  } else {
    df
  }
}

```

## Setup

### Load package with dependencies

```{r, packages}
# Load csn package with devtools
# Includes loading the following packages:
#   - tidyverse
#   - ggpubr
library(devtools)
load_all()
```

### File path definitions

```{r, file-path-definitions}
root_path <- "/Volumes/shlab/Projects/CSN"
extracted_data_path <- "/data/extracted/eyetracker"
event_csv <- "fevent.csv"
sample_csv <- "fsample.csv"
ioevent_csv <- "ioevent.csv"
recordings_csv <- "recordings.csv"

id_prefix <- "CSN"
id_length <- 3
min_id <- 1
max_id <- 57
```

### Define loading methods for CSVs

```{r}
get_path_to_id <- function(id) {
  padded_id <- stringr::str_pad(id, 3, pad = "0")
  participant_id <- stringr::str_c(id_prefix, padded_id)
  return (
    file.path(
      root_path, 
      extracted_data_path, 
      participant_id
    )
  )
}

get_id_vector <- function() {
  # Gets a vector of id integers for participants relative
  # to the existence of that participant's data (some participants
  # are not converted from raw MAT data to CSVs if not considered
  # a complete participant)
  id_vector <- c()
  for (i in min_id:max_id) {
    if ( dir.exists(get_path_to_id(i)) ) {
      id_vector <- c(id_vector, i)
    }
  }
  return (id_vector)
}

load_all_etd_by_filename_csv <-function(id_vector, filename_csv = sample_csv) {
  # Loads all eyetracking data of given filename across participants
  etd_list <- list()
  for (id in id_vector) {
    path_to_etd <- file.path(get_path_to_id(id), filename_csv)
    etd_list[[id]] <- readr::read_csv(path_to_etd)
  }
  return (etd_list)
}

# Convenience instantiation of the id vector for later use. Can use
# the getter method at any point for same output.
id_vector <- get_id_vector()
```

### Load each CSV for all participants' events and recordings data

```{r, message=FALSE}
etd_events <- load_all_etd_by_filename_csv(id_vector, event_csv)
etd_recordings <- load_all_etd_by_filename_csv(id_vector, recordings_csv)
```

## Events data

### Methods using `etd_events` list of loaded dataframes

```{r, events-data-methods}
CALIBRATION_RESULT_MESSAGE <- "!CAL CALIBRATION HV9 R RIGHT"
VALIDATION_RESULT_MESSAGE <- "!CAL VALIDATION HV9 R RIGHT"
CATEGORY_STRING_PATTERN <- "CALIBRATION|VALIDATION"
QUALITY_STRING_PATTERN <- "GOOD|FAIR|POOR"
AVG_ERROR_STRING_INDEX <- 8
MAX_ERROR_STRING_INDEX <- 10
DEG_OFFSET_STRING_INDEX <- 13
PIX_OFFSET_STRING_INDEX <- 15


get_category_from_message <- function(message) {
  # Extract the category of event message
  as.character(str_extract(message, CATEGORY_STRING_PATTERN))
}


get_quality_from_message <- function(message) {
  # Extract the quality of calibration or validation from event message
  as.character(str_extract(message, QUALITY_STRING_PATTERN))
}


get_avg_error_from_message <- function(message) {
  # Extract the avg error of validation
  as.double(word(message, AVG_ERROR_STRING_INDEX))
}


get_max_error_from_message <- function(message) {
  # Extract the max error of validation
  as.double(word(message, MAX_ERROR_STRING_INDEX))
}


get_deg_offset_from_message <- function(message) {
  # Extract the deg offset of validation
  as.double(word(message, DEG_OFFSET_STRING_INDEX))
}


get_pix_offset_from_message <- function(message) {
  # Extract the x coordinate of pix offset of validation
  word(message, PIX_OFFSET_STRING_INDEX)
}


get_event_messages <- function(participant_events) {
  # Get the significant event messages for a single participant
  # events data by sttime, producing the extracted rows and columns 
  # from message contents.
  
  if ("df" %in% ls()) rm("df")
  
  # Select desired columns and remove white space around message
  df <- participant_events %>%
    select(message, sttime) %>%
    transmute(message = str_squish(str_trim(message)), sttime = sttime) 
  
  # Extract rows from categorized event result messages
  df <- df %>%
    filter(str_detect(message, CALIBRATION_RESULT_MESSAGE) | str_detect(message, VALIDATION_RESULT_MESSAGE)) %>%
    mutate(
      category = get_category_from_message(message),
      quality = get_quality_from_message(message),
      avg_error = get_avg_error_from_message(message),
      max_error = get_max_error_from_message(message),
      deg_offset = get_deg_offset_from_message(message),
      pix_offset = get_pix_offset_from_message(message)
    ) 
  
  # Extract columns for pixel offsets of x,y coordinates
  df <- df %>%
    separate(pix_offset, c("pix_x_offset", "pix_y_offset"), ",") %>%
    mutate(
      pix_x_offset = as.double(pix_x_offset),
      pix_y_offset = as.double(pix_y_offset)
    ) 
  
  # Move message column to last column for convenient notebook 
  # reading of dataframe output
  df <- df %>%
    relocate(-message)
  
  return (df)
  
}
```

```{r}
# Output a single participant's important calibration/validation info
make_pretty_df(
  get_event_messages(etd_events[[5]])
)
```

## Recordings data

### Methods using `etd_recordings` list of loaded dataframes

```{r, recordings-data-methods}
get_duration <- function(id) {
  # Duration from highest start time and lowest start time, in minutes
  summary <- etd_recordings[[id]] %>% 
    summarize(duration = (max(time) - min(time)) / (1000*60) ) %>%
    unnest(cols = c())
  return(summary$duration)
}

get_durations <- function(id_vector) {
  # Durations in minutes for all participants events' data
  durations <- c()
  for (i in id_vector) {
    durations <- c(durations, get_duration(i))
  }
  return(durations)
}

get_recording_time_matrix <- function(id_vector, dimensional_reducer = 1) {
  # Get a built matrix with a row for each id in id_vector, with four
  # times per row. Optional dimensional reducer to achieve times in seconds, minutes.
  n_ids <- length(id_vector)
  n_recordings <- 1 + length(etd_recordings[[id_vector[[1]]]]$time)
  recording_time_matrix <- matrix(NA, nrow=n_ids, ncol=n_recordings)
  
  for (i in 1:n_ids) {
    id <- id_vector[i]
    recording_time_matrix[i,] <- c(id, (etd_recordings[[id]]$time / dimensional_reducer))
  }
  
  return(recording_time_matrix)
  
}

get_recording_time_df <- function(id_vector, dimensional_reducer = 1) {
  # Get dataframe using matrix of ids and times from recordings. Optional
  # dimensional reducer to achieve times in seconds, minutes.
  m <- get_recording_time_matrix(id_vector, dimensional_reducer)
  df <- as.data.frame(m)
  recording_time_df_cols <- c("id", "calibration", "validation", "task", "revalidation")
  colnames(df) <- recording_time_df_cols
  
  return(df)
  
}
```

### Look at recording times

Can retrieve all recording moments across participants by seconds or minutes. In seconds, the difference between revalidation and task time, then subtracting 3600, provides an idea of how much "overtime" the task went.

```{r}
recording_time_df_seconds <- get_recording_time_df(id_vector, 1000)
recording_time_df_minutes <- get_recording_time_df(id_vector, 1000 * 60)

sort(recording_time_df_seconds$revalidation - recording_time_df_seconds$task - 3600)
```

## Evaluate validation and revalidation quality

### Methods to create validation and revalidation dataframes

```{r, joint-data-methods}
get_val_reval_by_id <- function(recording_time_df, i) {
  
  recording_times_for_id <- recording_time_df %>%
    filter(id == i)
  
  all_validations_by_id <- get_event_messages(etd_events[[i]]) %>%
    filter(category == "VALIDATION") %>%
    select(-c(message, category))

  validation <- all_validations_by_id %>%
    filter(sttime < recording_times_for_id$task) %>%
    arrange(sttime) %>%
    slice_tail(n = 1) %>%
    mutate(id = as.integer(i)) %>%
    relocate(id) %>%
    rename_with(~ paste(.x, "val", sep = "_"), -id)
    
  revalidation <- all_validations_by_id %>%
    filter(sttime > (recording_times_for_id$task + 60 * 60 * 1000)) %>%
    arrange(sttime) %>%
    slice_tail(n = 1) %>%
    mutate(id = as.integer(i)) %>%
    relocate(id) %>%
    rename_with(~ paste(.x, "reval", sep = "_"), -id)
  
  left_join(
    validation,
    revalidation,
    by = c("id")
  )
  
}


get_all_val_reval_df <- function(id_vector) {
  
  if ("df" %in% ls()) rm("df")
  
  df <- data.frame()
  
  recording_time_df <- get_recording_time_df(id_vector)
  
  for (i in id_vector) {
    val_reval_df <- get_val_reval_by_id(recording_time_df, i)
    df <- bind_rows(df, val_reval_df)
  }
  
  return(df)
}

get_recording_and_val_reval_df <- function(recording_time_df, all_val_reval_df) {
  
  left_join(
    recording_time_df, 
    all_val_reval_df,
    by = c("id")
  )
  
}
```

#### Combine recording times with significant event details per participant

```{r, combine-recording-events, message=FALSE}
recording_time_df <- get_recording_time_df(id_vector)
all_val_reval_df <- get_all_val_reval_df(id_vector)

recording_and_val_reval_df <- get_recording_and_val_reval_df(recording_time_df, all_val_reval_df)
```

Visulaize the head of the dataframe:

```{r, show-table-recording-and-val-reval}
make_pretty_df(
  head(recording_and_val_reval_df, 10)
)
```

Mutate to get change in average error, max error, and pixel offsets. 

```{r}
val_reval_changes_df <- recording_and_val_reval_df %>%
    mutate(
      avg_error_change = avg_error_reval - avg_error_val,
      max_error_change = max_error_reval - max_error_val,
      pix_x_offset_change = pix_x_offset_reval - pix_x_offset_val,
      pix_y_offset_change = pix_y_offset_reval - pix_y_offset_val
    ) %>%
    relocate(c(avg_error_change, max_error_change, pix_x_offset_change, pix_y_offset_change), .after = id) %>%
    arrange(abs(avg_error_change))

make_pretty_df(
  tail(val_reval_changes_df, 10)
)
```

### Correlation tests for `avg_error` and `max_error`

Compare the average error between validation and revalidation across
participants using a Pearson's product-moment correlation. Do the same for max error.

```{r}
low_avg_error_df <- val_reval_changes_df %>%
  filter(avg_error_change < 7)

avg_error_corr <- cor.test(
  low_avg_error_df$avg_error_val, 
  low_avg_error_df$avg_error_reval,
  method = "pearson",
  use = "complete.obs"
)

max_error_corr <- cor.test(
  low_avg_error_df$max_error_val, 
  low_avg_error_df$max_error_reval,
  method = "pearson",
  use = "complete.obs"
)

avg_error_corr
max_error_corr
```

### Methods for correlation plots

```{r}
get_error_correlation_plot <- function(df, measure) {
  
  ggscatter(
    data = df, 
    x = str_glue("{ measure }_error_val"), 
    y = str_glue("{ measure }_error_reval"),
    add = "reg.line",
    add.params = list(color = "blue", fill = "lightgray"),
    conf.int = TRUE
  ) +
  xlim(0, 5) +
  ylim(0, 5) +
  coord_fixed(ratio = 1) +
  geom_abline() +
  stat_cor(method = "pearson") +
  theme_classic2() +
  labs(
    title = tools::toTitleCase(str_glue("Correlation of { measure } error between validation and revalidation")),
    x = tools::toTitleCase(str_glue("{ measure } Error of Validation")),
    y = tools::toTitleCase(str_glue("{ measure } Error of Revalidation"))
  )
  
}

get_offset_correlation_plot <- function(df, measure) {
  
  ggscatter(
    data = df, 
    x = str_glue("pix_{ measure }_offset_val"), 
    y = str_glue("pix_{ measure }_offset_reval"),
    add = "reg.line",
    add.params = list(color = "blue", fill = "lightgray"),
    conf.int = TRUE
  ) +
  stat_cor(method = "pearson") +
  theme_classic2() +
  labs(
    title = tools::toTitleCase(str_glue("Pixel offset ({ measure }) changes between validation and revalidation")),
    x = tools::toTitleCase(str_glue("{ measure } Offset in Validation (px)")),
    y = tools::toTitleCase(str_glue("{ measure } Offset in Revalidation (px)"))
  ) 
  
}

get_offset_boxplot <- function(df, group, measure) {
  
  ggboxplot(
    data = df,
    x = str_glue("{ group }"),
    y = str_glue("{ measure }"),
    color = str_glue("{ group }"),
    order = c("val", "reval")
  ) +
  theme_classic2() +
  labs(
    title = tools::toTitleCase(str_glue("Pixel offset ({ measure }) from validation to revalidation"))
  ) 
  
}

```

```{r}
get_error_correlation_plot(val_reval_changes_df, "avg")
```


```{r}
get_error_correlation_plot(val_reval_changes_df, "max")
```

### Paired samples tests for x,y pixel offsets

Create a categorized offset dataframe to use in boxplots.

```{r}
categorized_offset_df <- val_reval_changes_df %>%
  select(id, matches("pix"), -matches("change")) %>% # remove offset change columns
  pivot_longer(
    cols = -c(id), # don't select for id
    names_to = c(".value", "category"),
    names_pattern = "pix_(.)_offset_(.*)"
  ) %>%
  mutate(
    distance = sqrt(x^2 + y^2)
  )
make_pretty_df(
  head(categorized_offset_df)
)
```

#### Shapiro-Wilk normality tests

By results of Shapiro-Wilk normality test for the x offset (p < 0.05), we may
reject the normality hypothesis, so should use the paired samples Wilcoxon
test. Regarding y offset (p = 0.41), we can assume normality and run a paired
samples t-test.

```{r}
shapiro.test(val_reval_changes_df$pix_x_offset_change)
shapiro.test(val_reval_changes_df$pix_y_offset_change)
```

Then, using the categorized offset dataframe, apply test to distance
calculated via norming (x,y) and origin. It's very likely the distances
from betweeen validation and revalidation are not normally distributed (p << 0.05).

```{r}
shapiro.test(categorized_offset_df$distance)
```

#### Paired samples Wilcoxon test for x offset:

```{r}
wilcox.test(
  x = val_reval_changes_df$pix_x_offset_val, 
  y = val_reval_changes_df$pix_x_offset_reval,
  paired = TRUE
)
```

```{r}
get_offset_boxplot(df = categorized_offset_df, group = "category", measure = "x")
```

#### Paired samples t-test for y offset

```{r}
t.test(
  x = val_reval_changes_df$pix_y_offset_val, 
  y = val_reval_changes_df$pix_y_offset_reval,
  paired = TRUE
)
```

```{r}
get_offset_boxplot(df = categorized_offset_df, group = "category", measure = "y")
```

#### Paired samples Wilcoxon test for distance

```{r}
wilcox.test(
  distance ~ category,
  data = categorized_offset_df,
  paired = TRUE
)
```

```{r}
get_offset_boxplot(df = categorized_offset_df, group = "category", measure = "distance")
```

### Paired Plots

Create a categorized offset dataframe and then show and example of it.

```{r, categorized-offset-dataframe}
categorized_offset_df <- val_reval_changes_df %>%
  select(id, matches("pix"), -matches("change")) %>% # remove offset change columns
  pivot_longer(
    cols = -c(id), # don't select for id
    names_to = c(".value", "category"),
    names_pattern = "pix_(.)_offset_(.*)"
  ) %>%
  group_by(id) %>%
  mutate(
    distance = sqrt(x^2 + y^2)
  ) %>%
  mutate(
    x_change = x[category == "reval"] - x[category == "val"],
    y_change = y[category == "reval"] - y[category == "val"],
    distance_change = distance[category == "reval"] - distance[category == "val"]
  )

make_pretty_df(
  head(categorized_offset_df)
)
```

Create a categorized error dataframe and then show an example of it.

```{r, categorized-error-dataframe}
categorized_error_df <- val_reval_changes_df %>%
  select(id, matches("error"), -matches("change")) %>% # remove offset change columns
  pivot_longer(
    cols = -c(id), # don't select for id
    names_to = c(".value", "category"),
    names_pattern = "(.*)_error_(.*)"
  ) %>%
  group_by(id) %>%
  mutate(
    avg_change = avg[category == "reval"] - avg[category == "val"],
    max_change = max[category == "reval"] - max[category == "val"]
  )

make_pretty_df(
  head(categorized_error_df)
)
```

Look at the participants who have decreasing average errors over the course
of the task.

```{r}
make_pretty_df(
  categorized_error_df %>%
    filter(avg_change < 0)
)
```

### Paired plots for validation-revalidation changes

Plot the change in average error with gradient coloring for the slope
of average error change.

```{r, offset-change-paired-plots-method}
make_paired_plot <- function(categorized_df, measure) {
  # requires a categorized dataframe, the category column selector, and measure (str)
  plt <- categorized_df %>%
    ggplot(aes(x = factor(category, level = c("val", "reval")), y = !!sym(measure), group = id)) +
    geom_point(size = 1, alpha = 0.5) +
    geom_line(alpha = 0.6, aes(color = !!sym(str_glue("{ measure }_change")))) +
    scale_color_gradient2(low = "red", mid = "gray", high = "blue")
  return (plt)
}
```

#### Offset changes

```{r, offset-change-paired-plots}
make_paired_plot(categorized_offset_df, 
  measure = "x") +
  labs(
    title = "Change in X Offset between Validation and Revalidation",
    #subtitle = "Only Decreases in Average Error",
    x = "Event Category",
    y = "X Offset (pixels)"
  ) +
  theme_classic2()

make_paired_plot(categorized_offset_df, 
  measure = "y") +
  labs(
    title = "Change in Y Offset between Validation and Revalidation",
    #subtitle = "Only Decreases in Average Error",
    x = "Event Category",
    y = "Y Offset (pixels)"
  ) +
  theme_classic2()

make_paired_plot(categorized_offset_df, 
  measure = "distance") +
  labs(
    title = "Change in Distance Offset between Validation and Revalidation",
    #subtitle = "Only Decreases in Average Error",
    x = "Event Category",
    y = "Distance Offset (pixels)"
  ) +
  theme_classic2()
```

#### Average error changes

```{r}
make_paired_plot(categorized_error_df %>% filter(avg_change < 0),
  measure = "avg") +
  labs(
    title = "Change in Avg Error between Validation and Revalidation",
    subtitle = "Only Decreases in Average Error",
    x = "Event Category",
    y = "Avg Error"
  ) +
  theme_classic2()

make_paired_plot(categorized_error_df %>% filter(avg_change > 0),
  measure = "avg") +
  labs(
    title = "Change in Avg Error between Validation and Revalidation",
    subtitle = "Only Increases in Average Error",
    x = "Event Category",
    y = "Avg Error"
  ) +
  theme_classic2()

make_paired_plot(categorized_error_df %>% filter(id != 11),
  measure = "avg") +
  labs(
    title = "Change in Avg Error between Validation and Revalidation",
    subtitle = "Only Increases in Average Error",
    x = "Event Category",
    y = "Avg Error"
  ) +
  theme_classic2()

```

## Samples data

```{r, load-samples}
subset_id_vector <- id_vector[1:5] # just for example since huge load time
etd_samples <- load_all_etd_by_filename_csv(id_vector, sample_csv)
```

### Extract samples for task duration

```{r, task-gaze-samples}
FIXED_ID <- 39
MILLISECONDS_PER_HOUR <- 3600000
MILLISECONDS_PER_SECONDS <- 1000
GX_SAMPLE_ERROR_VALUE <- 10^7
GY_SAMPLE_ERROR_VALUE <- 10^7
SCREEN_WIDTH_PX <- 1280
SCREEN_HEIGHT_PX <- 1024
SCREEN_BIN_WIDTH_PX <- 16
SCREEN_CENTER_COORD <- c(SCREEN_WIDTH_PX / 2, SCREEN_HEIGHT_PX / 2)

# get the task start time from recordings data embedded in cleaned
# val_reval changes dataframe
task_start_time <- val_reval_changes_df %>%
  filter(id == FIXED_ID) %>%
  pull(task)

# Filter sample data per participant for the hour duration
# from start to end of task, then translate and transform
# frame of reference for time to start at 0 in seconds. Remove
# all error values for gaze sample and plot
task_gaze_df <- etd_samples[[FIXED_ID]] %>% 
  filter(time >= task_start_time & time < task_start_time + MILLISECONDS_PER_HOUR) %>%
  mutate(time = (time - time[[1]]) / MILLISECONDS_PER_SECONDS) %>%
  filter(gx < GX_SAMPLE_ERROR_VALUE & gy < GY_SAMPLE_ERROR_VALUE)

plt <- ggplot(task_gaze_df, aes(x = gx, y = gy)) +
  xlim(0, SCREEN_WIDTH_PX) +
  ylim(0, SCREEN_HEIGHT_PX) +
  theme_minimal() +
  labs(
    title = str_glue("Heatmap of gaze samples across task duration for CSN{ str_pad(FIXED_ID, 3, pad = '0') }")
  )

# add clock circle in
# flip all participants to same orientation (all left, or all right, but not both)

# Validate translations for pixel offset, point by point
# 1. clock on left
# 2. Cut data from non-clock side/half
# 3. Average distance from clock hand's tip per timestamp
# 4. Offset of x,y from val/reval is the translation, use mean of two values for each orientation
# 5. Compare distance from clock with and without correction by offset
# 6. Worth looking at quartiles of average distance per participant
# 7. Then plot average distance for each participant in a plot
# 
# See matlab task code to see where tip of clock hand is at every second
plt + geom_bin2d(binwidth = c(SCREEN_BIN_WIDTH_PX, SCREEN_BIN_WIDTH_PX))
```

## Playground

Things in progress

```{r}
make_pretty_df(
  val_reval_changes_df %>%
    filter(avg_error_val < 1.5 & avg_error_reval < 1.5)
)
```


```{r}
df <- val_reval_changes_df %>%
    filter(avg_error_val < 2.5 & avg_error_reval < 2.5)

t.test(df$avg_error_change, mu = 0)
```
